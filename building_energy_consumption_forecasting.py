# -*- coding: utf-8 -*-
"""Building energy consumption forecasting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/samir1206/samir1206/blob/main/Building_energy_consumption_forecasting.ipynb

#**Objective**

Predicting energy budget of a building for a month
- Weather data for 3 years
- Energy consumption record of 3 years
- Energy cost of a month
"""

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""Load the building electricity consumption data"""

Energy1 = pd.read_excel('Building energy consumption racord (1) (1)')
Energy1

Energy = Energy1.set_index('Time')
Energy

Energy.describe()

plt.boxplot(Energy['building 12'])
plt.show()

import seaborn as sns
sns.boxplot(Energy['building 12'])

#Load the Weather data from the excel file
knmi= pd.read_excel('WeatherData (2).xlsx')
knmi

#Set the Time column as index
knmi = knmi.set_index("Time")
knmi

#concatenating the datasets of weather data and electricity consumption
df = pd.concat([knmi, Energy], axis=1) #axis =1 for considering the columns
df

df.isna().sum()

import missingno as msno
msno.bar(df)

"""We have no missing values in the dataframe. Our output (dependent variable) is building 12 column. other columns of the data set are independent values. We can find strong and weak correlation with different variables. With .corr() method, we can utilize Pearson's correlation coefficient which is a measure of the strength of a linear association between two variables."""

plt.figure(figsize = (16,6))
sns.heatmap(df.corr(), annot = True, linewidths=1, fmt=".2g", cmap= 'coolwarm')

plt.xticks(rotation='horizontal')

"""From the heatmap, we see temperature (Temp) correlates very positively with building electricity demand. Relative humidity (U) and hourly sum of precipitation (RH) are two highest negatively correlated features. in addition, both of these features are also multi-collinear. Which means, either of them can be utilized for predicting electricity demand.
#plot energy consumption data against U and Temp
"""

df_sum_weekly = df['building 12'].resample('W').mean()


df_feature1= df["Temp"].resample("W").mean()

df_feature2 = df["U"].resample("W").mean()

fig,ax = plt.subplots(figsize=(24,8))
ax.plot(df_sum_weekly.index, df_sum_weekly, color="red",marker="o")
ax.set_ylabel("KWh")
ax.set_xlabel('Date')
ax2 = ax.twinx()
ax3 = ax.twinx()
ax2.plot(df_sum_weekly.index, df_feature1, color="blue", marker="o")
ax2.set_ylabel("°C")
ax3.plot(df_sum_weekly.index, df_feature2, color="green", marker="o")
ax3.set_ylabel("grams/cubic meter")
ax3.spines["right"].set_position(("axes", 1.05))
fig.legend(["Resampled Weekly Energy Consumption","Resampled Weekly Temperature","Resampled Weekly Relative Humidity"], loc='upper right')
fig.show()

"""We see that energy demand of a building varies with temperature. Variations of the energy consumption across various seasons are also visible.
Negative linear correlation of Relative Humidity can be explained. It is not just correlational with Energy consumption but also has high negative correlation (-0.57) with temperature. The correlations observed are well expected.

## Feature selection
We can now select features based on their strong coorealtion with the output and remove some input features which are strongly coorelated with each other to avoid the problem of multicolinearity. It is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.

### Exploring non-linear correlation between Energy with Hour and Month
We use spearman's correlation between two variables. The **Spearman rank-order correlation coefficient** is a nonparametric measure of the monotonicity of the relationship between two datasets. Unlike the Pearson correlation, the Spearman correlation does not assume that both datasets are normally distributed. This one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.
"""

from scipy.stats import spearmanr


energy = np.array(df["building 12"])
hour = np.array(df["HH"])
month= np.array(df["month"])


corr1, _ = spearmanr(energy, hour)
corr2,_ = spearmanr(energy, month)
print('Spearmans correlation between Energy and hour feature: %.3f' % corr1)
print('Spearmans correlation between Energy and month feature: %.3f' % corr2)

"""We see, the energy consumption has a seasonal effect which is reflected on the different months of the year. So, it has more correlation with month than hours of the day."""

knmi_updated= knmi.loc[:, ~knmi.columns.isin(["TD","U","DR","FX"])]

"""### Now we develop a machine learning regression model based on the weather parameters to predict the energy consumption of the building.
Various forecasting techniques can be utilized with machine learning models. (Deng et al., 2018) tested the performance of various machine learning models on one of the largest database on buildings in CBECS, and found both Support Vector Machine (SVM) and Random Forest (RF) being able to handle the non-linear relationships better as they perform dynamic local investigations better rather than global optimization. Therefore, we are going to consider SVM and RF to develop the model.  
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(knmi_updated, Energy, test_size = 0.2, random_state = 0)
y_train

y_train = y_train.values.ravel()
y_train

y_test = y_test.values.ravel()
y_test

from sklearn.svm import SVR


SVReg = SVR(kernel= 'rbf')


SVReg.fit(X_train, y_train)

"""Three hyperparameters for the SVR function are Epsilon, C and Gamma.

Epsilon represents the **maximum error allowed in the function**, and determines the desired accuracy of the model. A smaller ϵ indicates that a more accurate model is required.

Parameters C is used to **set the tolerance for points which fall outside of the error boundaries set by ϵ**. A larger value for C indicates a larger tolerance for points outside of ϵ.

The gamma parameter defines the **influence of a single point on the model**. A low value indicates that points have a far reach, meaning that also points that are situated far away from the regression line are taken into account. A low value results in a more linear regression line. On the other hand, a high value for gamma indicates that points have a close reach, which results in a more complex or wobbly line around the nearby points. With default 'scale', the value of gamma is 1 / (n_features * X.var()).
"""

Predicted_Train= SVReg.predict(X_train)
Predicted_Train

from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error

print(r2_score(y_train,Predicted_Train))
print(mean_squared_error(y_train,Predicted_Train))

"""##Scaling to improve the model performance
Scaling is used to bring all features to the same level of magnitudes. Without scaling, the features with high magnitudes will have more weight in the ‘best fit’ calculation, which tries to minimize the distance between the fit line and the observed values (Asaithambi, 2017).
"""

from sklearn.preprocessing import StandardScalerhe data
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import RobustScaler


sc1= StandardScaler()
sc2= MinMaxScaler()
sc3= RobustScaler()

X1 = sc1.fit_transform(knmi_updated)
X2 = sc2.fit_transform(knmi_updated)
X3 = sc3.fit_transform(knmi_updated)

sns.distplot(X1,color="red",label="StdScaler")
sns.distplot(X2,color="blue",label="MinMax")
sns.distplot(X3,color="black",label="RobustScaler")
plt.legend()

X_train, X_test, y_train, y_test = train_test_split(X1, Energy, test_size=0.2, random_state=0, shuffle= "False")
y_train = y_train.values.ravel()
y_test = y_test.values.ravel()

regr = SVR(kernel='rbf')
regr= regr.fit(X_train, y_train)
regr

regr.fit(X_train, y_train)

predict_train= regr.predict(X_train)

print(r2_score(y_train, predict_train))
print(mean_squared_error(y_train, predict_train))

pred= regr.predict(X_test)

print(r2_score(y_test, pred))
print(mean_squared_error(y_test, pred))

"""Now we test for MinMax scaling"""

X_train, X_test, y_train, y_test = train_test_split(X2, Energy, test_size=0.2, random_state=0, shuffle= "False")
y_train = y_train.values.ravel()
y_test = y_test.values.ravel()

regr = SVR(kernel='rbf')
regr= regr.fit(X_train, y_train)


regr.fit(X_train, y_train)

predict_train= regr.predict(X_train)


print(r2_score(y_train, predict_train))
print(mean_squared_error(y_train, predict_train))

pred= regr.predict(X_test)

print(r2_score(y_test, pred))
print(mean_squared_error(y_test, pred))

"""Finally, we test for Robust scaling"""

X_train, X_test, y_train, y_test = train_test_split(X3, Energy, test_size=0.2, random_state=0, shuffle= "False")
y_train = y_train.values.ravel()
y_test = y_test.values.ravel()


regr = SVR(kernel='rbf')
regr= regr.fit(X_train, y_train)


regr.fit(X_train, y_train)

predict_train= regr.predict(X_train)


print(r2_score(y_train, predict_train))
print(mean_squared_error(y_train, predict_train))

pred= regr.predict(X_test)
print(r2_score(y_test, pred))
print(mean_squared_error(y_test, pred))

"""We observe that, when the R2 value increases and RMS error decreases from the previous model, we get a better performing model. Therefore, Standard scaler is the best fit for our model which can explain 86.78% of the variance of the training dataset and 86.52% of the variance of the test dataset. The prediction accuracy will vary +-2.3 (root mean squared error of 5.5).
##Same way we can compare between different karnels
"""

X_train, X_test, y_train, y_test = train_test_split(X1, Energy, test_size=0.2, random_state=0, shuffle= "False")
y_train = y_train.values.ravel()
y_test = y_test.values.ravel()


regr = SVR(kernel='poly', degree=5)


regr.fit(X_train, y_train)

predict_train= regr.predict(X_train)


print(r2_score(y_train, predict_train))
print(mean_squared_error(y_train, predict_train))

"""###We see, rbf (radial basis Function) kernel performed better.
Now we can check the performance of the model by introducing other features which we removed earlier. The weather parameters used did not include the dew point temperature (TD), relative humidity (U), Duration of perception (DR) and Wind Gust (FX). We have assessed with lower number of varaibles. Adding extra variables may improve the performance of the model.
##Dont forget to check the variables in use.
Only parameter U is left out of the regression model, as U and RH (Hourly Sum of Precipitation) have a correlation of 100%. As a result, including only one of the two parameters in the model is sufficient.
"""

X4 = sc1.fit_transform(knmi.loc[:, ~knmi.columns.isin(["U"])])

X_train, X_test, y_train, y_test = train_test_split(X4, Energy, test_size=0.2, random_state=0, shuffle= "False")
y_train = y_train.values.ravel()
y_test = y_test.values.ravel()


regr = SVR(kernel='rbf')
regr= regr.fit(X_train, y_train)


regr.fit(X_train, y_train)

predict_train= regr.predict(X_train)


print(r2_score(y_train, predict_train))
print(mean_squared_error(y_train, predict_train))

pred= regr.predict(X_test)

print(r2_score(y_test, pred))
print(mean_squared_error(y_test, pred))

"""We see the improvement of the prediction accuracy. A model in which all weather variables are taken into account returns the best results, despite the low correlation between the added parameters with the energy demand."""

plt.figure(figsize = (20,5))
plt.plot(y_test, label="Original")
plt.plot(pred, label="Predicted")
plt.legend(loc='best')
plt.xlabel('Time (Hours)')
plt.ylabel('kWh')

"""Now we can further improve the perofrmance of the model by finding suitable hyperparameters (epsilon, C and gamma). We utilize gridsearch library for exhaustive search over specified parameter values for an estimator. Default settings for C, Epsilon and Gamma are 1, 0.1 and ’scale’. With best parameters, we can check the improved performance of the model."""

from sklearn.model_selection import GridSearchCV

check_parameters = {'C':[10,20,30], 'epsilon':[0.03, 0.5, 1], 'gamma':[5,6,7]}

gridsearchcv = GridSearchCV(regr, check_parameters, n_jobs=-1, cv=3)
gridsearchcv.fit(X_train, y_train)

print('Best parameters found:\n', gridsearchcv.best_params_)

#
Regr = SVR(kernel= 'rbf', C=30, epsilon = 0.03, gamma = 5)


regr.fit(X_train, y_train)
predict_train= regr.predict(X_train)


print(r2_score(y_train, predict_train))
print(mean_squared_error(y_train, predict_train))

Regr = SVR(kernel= 'rbf', C=40, epsilon = 0.03, gamma = 5)


regr.fit(X_train, y_train)

predict_train= regr.predict(X_train)


print(r2_score(y_train, predict_train))
print(mean_squared_error(y_train, predict_train))

"""We see the adjusted hyper aprameter performs better than the default settings.
###Check the RF regressor model performance
"""

from sklearn.ensemble import RandomForestRegressor


RFReg = RandomForestRegressor(max_depth=10, random_state=0)


X_train2, X_test2, y_train2, y_test2 = train_test_split(X1, Energy, test_size=0.2, random_state=0, shuffle= "False")
y_train2 = y_train2.values.ravel()
y_test2 = y_test2.values.ravel()
RFReg.fit(X_train2, y_train2)


Predicted_Train2= RFReg.predict(X_train2)


print(r2_score(y_train2, Predicted_Train2))
print(mean_squared_error(y_train2, Predicted_Train2))

from sklearn.ensemble import RandomForestRegressor


RFReg = RandomForestRegressor(max_depth=10, random_state=0)


X_train2, X_test2, y_train2, y_test2 = train_test_split(X4, Energy, test_size=0.2, random_state=0, shuffle= "False")
y_train2 = y_train2.values.ravel()
y_test2 = y_test2.values.ravel()
RFReg.fit(X_train2, y_train2)

Predicted_Train2= RFReg.predict(X_train2)


print(r2_score(y_train2, Predicted_Train2))
print(mean_squared_error(y_train2, Predicted_Train2))

plt.plot(y_train2, color="b",label= 'Original Data')
plt.plot(Predicted_Train2, color ="red", label="RFReg predicted data")
plt.xlabel('Time (Hours)')
plt.ylabel('kWh')
plt.legend(loc='best')
plt.show()

Predicted_Test2 = RFReg.predict(X_test2)


print(r2_score(y_test2,Predicted_Test2))
print(mean_squared_error(y_test2,Predicted_Test2))

plt.plot(y_test2, color='blue', label="Actual")
plt.plot(Predicted_Test2, color='Red', label="Predicted")
plt.xlabel('Time (Hours)')
plt.ylabel('kWh')
plt.legend(loc='best')

from sklearn.model_selection import GridSearchCV

check_parameters = {'max_depth':[8,9,11,12]}

gridsearchcv = GridSearchCV(RFReg, check_parameters, n_jobs=-1, cv=3)
gridsearchcv.fit(X_train, y_train)

print('Best parameters found:\n', gridsearchcv.best_params_)

from sklearn.ensemble import RandomForestRegressor


RFReg = RandomForestRegressor(max_depth=12, random_state=0)


X_train2, X_test2, y_train2, y_test2 = train_test_split(X4, Energy, test_size=0.2, random_state=0, shuffle= "False")
y_train2 = y_train2.values.ravel()
y_test2 = y_test2.values.ravel()
RFReg.fit(X_train2, y_train2)


Predicted_Train2= RFReg.predict(X_train2)


print(r2_score(y_train2, Predicted_Train2))
print(mean_squared_error(y_train2, Predicted_Train2))

Predicted_Test2 = RFReg.predict(X_test2)


print(r2_score(y_test2,Predicted_Test2))
print(mean_squared_error(y_test2,Predicted_Test2))

#settings for hyperparameters
check_parameters = {'max_depth':[15,20,30]}

gridsearchcv = GridSearchCV(RFReg, check_parameters, n_jobs=-1, cv=10)
gridsearchcv.fit(X_train, y_train)

print('Best parameters found:\n', gridsearchcv.best_params_)

#importing the ensemble module for the random forest regressor from sklearn library
from sklearn.ensemble import RandomForestRegressor

# Creating an instance of the random forest regressor
RFReg = RandomForestRegressor(max_depth=30, random_state=0)

# fitting the regression model to the training data
X_train2, X_test2, y_train2, y_test2 = train_test_split(X4, Energy, test_size=0.2, random_state=0, shuffle= "False")
y_train2 = y_train2.values.ravel()
y_test2 = y_test2.values.ravel()
RFReg.fit(X_train2, y_train2)

#Predicting on the training data
Predicted_Train2= RFReg.predict(X_train2)

#Caculating R2 score and Root mean square error
print(r2_score(y_train2, Predicted_Train2))
print(mean_squared_error(y_train2, Predicted_Train2))

Predicted_Test2 = RFReg.predict(X_test2)


print(r2_score(y_test2,Predicted_Test2))
print(mean_squared_error(y_test2,Predicted_Test2))

"""#Allocate budget using predictive modeling
With the help of energy price and predicted demand, we can calculate the estimated cost of energy for the month of January. Now we have a trained model.
"""

path= "/content/drive/MyDrive/Colab Notebooks/Building energy forecasting/Weather_Cost.xlsx"
weather_cost = pd.read_excel(path)
weather_cost

weather_cost = weather_cost.set_index('Time')

weather_cost.isna().sum()

weather_cost_updated= weather_cost.loc[:, ~weather_cost.columns.isin(['U'])]

X5 = sc1.transform(weather_cost_updated)

predicted = RFReg.predict(X5)
predicted.shape

predicted= pd.DataFrame(predicted, columns=['kWh'])
predicted

predicted['Time']= weather_cost.index
predicted

predicted= predicted.set_index('Time')
predicted

fig, ax = plt.subplots(figsize = (16,5))
ax.plot(predicted,  label='Hourly Predicted consumption',color = 'blue')
ax.set_ylabel('Hourly Predicted consumption [kWh]',size=15, color='green')
ax.set_xlabel('Time',size=15)

"""#Plot the hourly forecast consumption in kWh and calculated price for the whole month"""

Daily_Cost = Hourly_Cost.resample("D").sum()

print("total cost", Daily_Cost.sum())

fig, ax = plt.subplots(figsize = (16,5))
ax2 = ax.twinx()
ax.plot(Hourly_Cost,  label='Hourly Price',color = 'tab:red')
ax2.plot(predicted,  label='Hourly Predicted consumption',color = 'tab:green')
ax.set_ylabel('Hourly Price [Rupees]', size=16, color='orange')
ax2.set_ylabel('Hourly Predicted consumption [kWh]',size=16, color='green')
ax.set_xlabel('Time',size=16,)
fig.legend()

fig, ax = plt.subplots(figsize = (16,5))
ax2 = ax.twinx()
ax.plot(Hourly_Cost,  label='Hourly Price',color = 'tab:red')
ax2.plot(predicted,  label='Hourly Predicted consumption',color = 'tab:green', linestyle='dashed')
ax.set_ylabel('Hourly Price [Rupee]', size=16, color='orange')
ax2.set_ylabel('Hourly Predicted consumption [kWh]',size=16, color='green')
ax.set_xlabel('Time',size=16)
fig.legend()

fig, ax = plt.subplots(figsize=(16,5))
ax2 = ax.twinx()
ax.plot(Daily_Cost, label= 'Daily price', color = 'tab:orange')
ax2.plot(predicted, label='Hourly predicted consumption', color = 'tab:green')
ax.set_ylabel('Daily Price [Euro]', size=15, color='orange')
ax2.set_ylabel('Hourly Predicted consumption [kWh]',size=15, color='green')
ax.set_xlabel('Time',size=15)
fig.legend()

"""##Set a visual threshold in the forecast when the model predicts higher than a budget limit"""

fig = plt.figure(figsize = (16,5))
ax = fig.add_subplot(111)
Daily_Cost.plot(kind='bar', ax=ax, rot=0,color='green')
ax.axhline(y=160, color='red', linestyle='--', label="Warning for over budget ")
plt.text(17, 165, 'Warning for over budget', fontsize=12)

ax.set_ylabel('Consumption', size=16, color='black')
plt.xticks(rotation='vertical')
ax.set_xticklabels([dt.strftime('%Y-%m-%d') for dt in Daily_Cost.index])
fig.legend()